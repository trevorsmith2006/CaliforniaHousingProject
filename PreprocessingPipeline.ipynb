{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning  \n",
    "In this notebook, we will prepare the data for a Maching Learning algorithm. We will develop a preprocessing pipeline that does the following: \n",
    "1. adds useful attributes\n",
    "2. Deals with missing values\n",
    "3. Encodes any non-numerical data\n",
    "4. Rescales our features to an appropriate range and variance for our algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "\n",
    "# graphing\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import our stratified training set from the ExploratoryAnalysis notebook\n",
    "housing = pd.read_pickle(\"StratifiedTrainingSet.pkl\")\n",
    "\n",
    "# only keep the attributes we're interested in\n",
    "attributes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.1851e+02  3.4260e+01  2.9000e+01  2.1195e+03  4.3300e+02  1.1640e+03\n",
      "  4.0800e+02  3.5409e+00  1.7950e+05]\n",
      "[-1.1851e+02  3.4260e+01  2.9000e+01  2.1195e+03  4.3300e+02  1.1640e+03\n",
      "  4.0800e+02  3.5409e+00  1.7950e+05]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Imputing Missing Numerical Data\n",
    "\n",
    "for missing values of any attribute, we will use an imputer to fill in the median value\n",
    "this only works for numerical values, so we will drop ocean_proximity\n",
    "the only attribute that is missing values here is total_bedrooms, but our solution is general\n",
    "\"\"\"\n",
    "\n",
    "# import and create an imputer object\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "# drop our categorical attribute\n",
    "housing_num = housing.drop(columns=[\"ocean_proximity\"])\n",
    "\n",
    "# 'fit' the imputer to the data (given our strategy, it calculates the median)\n",
    "imputer.fit(housing_num)\n",
    "\n",
    "# check that the imputer calculated the median\n",
    "print(imputer.statistics_)\n",
    "print(housing_num.median().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the imputer to transform the data and fill missing data.\n",
    "# returns a numpy array\n",
    "X = imputer.transform(housing_num)\n",
    "\n",
    "# convert back to pandas dataframe\n",
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns, index=housing_num.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]]\n",
      "[array(['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],\n",
      "      dtype=object)]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Encoding Categorical Data\n",
    "\n",
    "We will encode our ocean_proximity text data to be numerical\n",
    "Since there are only 5 categories, and they are not necessarily \"adjacent\" to one another in the way integers\n",
    "are, we will use one-hot encoding.\n",
    "This will convert our 1 categorical variable into 5 boolean variables. Each data entry will have a value of 1\n",
    "for one of the resulting columns, and 0 for all others.\n",
    "\"\"\"\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# get the column of categorical data\n",
    "housing_cat = housing[[\"ocean_proximity\"]]\n",
    "\n",
    "# create encoder object\n",
    "cat_encoder = OneHotEncoder()\n",
    "\n",
    "# fit the encoder to the data and transform the data at the same time, using fit_transform\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
    "\n",
    "print(housing_cat_1hot.toarray()[:10])\n",
    "print(cat_encoder.categories_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.19575834e+02  3.56395773e+01  2.86531008e+01  2.62272832e+03\n",
      "  5.33998123e+02  1.41979082e+03  4.97060380e+02  3.87558937e+00\n",
      "  2.06990921e+05]\n",
      "[4.00720174e+00 4.57101322e+00 1.58114157e+02 4.57272746e+06\n",
      " 1.68778972e+05 1.24468040e+06 1.41157604e+05 3.62861320e+00\n",
      " 1.33863769e+10]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Feature Scaling\n",
    "\n",
    "We will employ a standardized scaling approach to the data, since many of our attributes\n",
    "have long positive tails (i.e. many positive outliers)\n",
    "\"\"\"\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit the scaler to the data (finds mean and variance)\n",
    "scaler.fit(housing_tr)\n",
    "\n",
    "# print calculated values for each column\n",
    "print(scaler.mean_)\n",
    "print(scaler.var_)\n",
    "\n",
    "# transform the data\n",
    "housing_scaled = scaler.transform(housing_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Pipeline  \n",
    "using a scikit-learn pipeline, we will combine and automate all the preprocessing steps that we have designed so far.  \n",
    "1. create new attributes (rooms per household, bedrooms per room, population per household)\n",
    "2. replace missing values with the median statistic for that column\n",
    "3. encode the ocean_proximity categorical variable using the one-hot method\n",
    "4. scale all numerical data with standardized scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a transformer class we can use to add our new attributes\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# store the column indices we will use to make new attributes\n",
    "rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n",
    "\n",
    "# note: do not add *args or **kargs for BaseEstimator\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, X):\n",
    "        return self # nothing else to do\n",
    "    def transform(self, X):\n",
    "        rooms_per_household = X[:,rooms_ix]/X[:,households_ix]\n",
    "        bedrooms_per_room = X[:,bedrooms_ix]/X[:,rooms_ix]\n",
    "        population_per_household = X[:,population_ix]/X[:,households_ix]\n",
    "        \n",
    "        return np.c_[X, rooms_per_household, bedrooms_per_room, population_per_household]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pipeline for our numerical attributes only\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('attribs_adder', CombinedAttributesAdder()),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PreprocessingPipeline.pkl']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine the numerical pipeline with the categorical one (just one-hot encoding)\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_attribs),\n",
    "    (\"cat\", OneHotEncoder(), cat_attribs)\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
